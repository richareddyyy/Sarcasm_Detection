{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle\n",
    "import operator\n",
    "import numpy as np\n",
    "import preprocessing as pp\n",
    "\n",
    "global char_n_grams_index, word_n_grams_index, truth_top_hashtags, truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens\n",
    "# Add features derived from emoticons.\n",
    "def addemoticonfeatures(feature_vector, emoticons):\n",
    "    feature_vector.append(len(emoticons))\n",
    "\n",
    "    for i in xrange(len(pp.all_emoticons)):\n",
    "        if pp.all_emoticons[i] in emoticons:\n",
    "            feature_vector.append(1)\n",
    "        else:\n",
    "            feature_vector.append(0)\n",
    "    return feature_vector\n",
    "\n",
    "def addchargramfeatures(feature_vector, char_n_grams_index, char_n_grams):\n",
    "    char_features = [0] * len(char_n_grams_index)\n",
    "    for char_i_gram in char_n_grams:\n",
    "        if char_i_gram in char_n_grams_index:\n",
    "            char_features[char_n_grams_index[char_i_gram]] = 1\n",
    "    feature_vector.extend(char_features)\n",
    "    return feature_vector\n",
    "\n",
    "def addwordfeatures(feature_vector, word_n_grams_index, word_n_grams):\n",
    "    word_features = [0] * len(word_n_grams_index)\n",
    "\n",
    "    for word_i_gram in word_n_grams:\n",
    "        if word_i_gram in word_n_grams_index:\n",
    "            word_features[word_n_grams_index[word_i_gram]] = 1\n",
    "    feature_vector.extend(word_features)\n",
    "    return feature_vector\n",
    "\n",
    "def addtoptokenfeatures(feature_vector, top_hi_tokens, top_en_tokens, top_rest_tokens, tweet):\n",
    "    for i in xrange(len(top_hi_tokens)):\n",
    "        if top_hi_tokens[i].lower() in tweet.lower():\n",
    "            feature_vector.append(1)\n",
    "        else:\n",
    "            feature_vector.append(0)\n",
    "    for i in xrange(len(top_en_tokens)):\n",
    "        if top_en_tokens[i].lower() in tweet.lower():\n",
    "            feature_vector.append(1)\n",
    "        else:\n",
    "            feature_vector.append(0)\n",
    "    for i in xrange(len(top_rest_tokens)):\n",
    "        if top_rest_tokens[i].lower() in tweet.lower():\n",
    "            feature_vector.append(1)\n",
    "        else:\n",
    "            feature_vector.append(0)\n",
    "    return feature_vector\n",
    "\n",
    "def buildtruthfeaturevector(key, tweet):\n",
    "    global char_n_grams_index, word_n_grams_index, truth_top_hashtags, truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens\n",
    "\n",
    "    emoticons, hashtags, mentions, urls, char_n_grams, word_n_grams = pp.preprocess(key, tweet)\n",
    "\n",
    "    truth_feature_vector = []\n",
    "\n",
    "    truth_feature_vector = addemoticonfeatures(truth_feature_vector, emoticons)\n",
    "    truth_feature_vector = addchargramfeatures(truth_feature_vector, char_n_grams_index, char_n_grams)\n",
    "    truth_feature_vector = addwordfeatures(truth_feature_vector, word_n_grams_index, word_n_grams)\n",
    "    truth_feature_vector = addtoptokenfeatures(truth_feature_vector, truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens, tweet)\n",
    "    return truth_feature_vector\n",
    "\n",
    "# Build feature vector for a given tweet.\n",
    "# 1. No. of emoticons and occurence for each emoticon.\n",
    "# 2. Char n-grams (n=1-3).\n",
    "# 3. Word n-grams (n=1-5).\n",
    "# 4. Target tokens.\n",
    "def getfeaturevector(key, tweet):\n",
    "    global char_n_grams_index, word_n_grams_index, truth_top_hashtags, truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens\n",
    "\n",
    "    fp = open('data.txt', 'r')\n",
    "    data = []\n",
    "    for i in xrange(pickle.load(fp)):\n",
    "        data.append(pickle.load(fp))\n",
    "    char_n_grams_index, word_n_grams_index, truth_top_hashtags, truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens = data\n",
    "    truth_feature_vector = buildtruthfeaturevector(key, tweet)\n",
    "    \n",
    "    return truth_feature_vector\n",
    "\n",
    "# tweet = \"Tuits Tsunami! Optimistic about the future? #Elecciones #ComunicaciónPolítica #VamosJuntos #LlamadasQueUnen #CaminemosJuntos #Cambiemos #27S\"\n",
    "# getfeaturevector(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

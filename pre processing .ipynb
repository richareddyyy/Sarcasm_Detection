{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from extract_tweets import get_tweet_map, get_id_tokenised_tweet_map, get_stop_words\n",
    "\n",
    "global n_for_char, n_for_word\n",
    "n_for_char = 3\n",
    "n_for_word = 5\n",
    "\n",
    "all_emoticons = \\\n",
    "[':-)', ':)', '(:', '(-:',\\\n",
    "':-D', ':D', 'X-D', 'XD', 'xD',\\\n",
    "'<3', ':\\*',\\\n",
    "';-)', ';)', ';-D', ';D', '(;', '(-;',\\\n",
    "':-(', ':(',\\\n",
    "':,(', ':\\'(', ':\"(', ':((',\\\n",
    "':-P', ':P', ':p', ':-p',\\\n",
    "]\n",
    "\n",
    "# Get emoticons from a tweet.\n",
    "def getemoticons(tweet):\n",
    "    emoticons = []\n",
    "    for emoticon in all_emoticons:\n",
    "        if emoticon in tweet:\n",
    "            emoticons.append(emoticon)\n",
    "    return emoticons\n",
    "\n",
    "# Get hashtags from a tweet.\n",
    "def gethashtags(tokenized_tweet):\n",
    "    hashtags = []\n",
    "    for token in tokenized_tweet:\n",
    "        if token[0] == '#':\n",
    "            hashtags.append(token.lower())\n",
    "    return hashtags\n",
    "\n",
    "# Get mentions from a tweet.\n",
    "def getmentions(tokenized_tweet):\n",
    "    mentions = []\n",
    "    for token in tokenized_tweet:\n",
    "        if token[0] == '@':\n",
    "            mentions.append(token)\n",
    "    return mentions\n",
    "\n",
    "# Get URLs from a tweet.\n",
    "def geturls(tweet):\n",
    "    url_regex = [r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+']\n",
    "    url_re = re.compile(r'('+'|'.join(url_regex)+')', re.VERBOSE | re.IGNORECASE)\n",
    "    urls = url_re.findall(tweet)\n",
    "    return urls\n",
    "\n",
    "# Get character n-grams (n=1-3) for a tweet.\n",
    "def getcharngrams(tweet):\n",
    "    char_n_grams = []\n",
    "    for i in xrange(1, n_for_char + 1):\n",
    "        char_i_grams = [tweet[j:j+i] for j in xrange(len(tweet)- (i-1))]\n",
    "        char_n_grams.extend(char_i_grams)\n",
    "    return char_n_grams\n",
    "\n",
    "# Filter a tweet by replacing the original hashtags, mentions, URLs and emoticons and removing punctuations, stop-words.\n",
    "def processtweet(tokenized_tweet, stop_words):\n",
    "processed_tweet = []\n",
    "\n",
    "    for i in xrange(len(tokenized_tweet)):\n",
    "        if tokenized_tweet[i][0] == '#':\n",
    "            processed_tweet.append(tokenized_tweet[i][1:])\n",
    "        elif tokenized_tweet[i][0] == '@' or tokenized_tweet[i][0] in string.punctuation:\n",
    "            continue\n",
    "        elif tokenized_tweet[i].lower() in stop_words:\n",
    "            continue\n",
    "        elif 'http' in tokenized_tweet[i]:\n",
    "            continue\n",
    "    for i in xrange(len(processed_tweet)):\n",
    "        processed_tweet[i] = processed_tweet[i].lower()\n",
    "    return processed_tweet\n",
    "\n",
    "# Get word n-grams (n=1-5) for a tweet.\n",
    "def getwordngrams(processed_tokenized_tweet):\n",
    "    word_n_grams = []\n",
    "    for i in xrange(1, n_for_word + 1):\n",
    "        word_i_grams = [\" \".join(processed_tokenized_tweet[j:j+i]) for j in xrange(len(processed_tokenized_tweet) - (i-1))]\n",
    "        word_n_grams.extend(word_i_grams)\n",
    "    return word_n_grams\n",
    "\n",
    "# Get count of all punctuations in a tweet.\n",
    "def getpunctuations(processed_tweet):\n",
    "    pucntuations_count = {}\n",
    "\n",
    "    for char in processed_tweet:\n",
    "        if char in all_punctuations:\n",
    "            if char in pucntuations_count:\n",
    "                pucntuations_count[char] += 1\n",
    "            else:\n",
    "                pucntuations_count[char] = 1\n",
    "    return pucntuations_count\n",
    "\n",
    "# Extract all the features of a tweet and create a feature vector.\n",
    "def preprocess(key, tweet):\n",
    "    id_tweet_map, tweet_id_map = get_tweet_map()\n",
    "    id_tokenised_tweet_map = get_id_tokenised_tweet_map()\n",
    "\n",
    "    tokenized_tweet = id_tokenised_tweet_map[key]\n",
    "    \n",
    "# Get emoticons, hashtags, mentions and URLs for a given tweet.\n",
    "    emoticons = getemoticons(tweet)\n",
    "    hashtags = gethashtags(tokenized_tweet)\n",
    "    mentions = getmentions(tokenized_tweet)\n",
    "    urls = geturls(tweet)\n",
    "\n",
    "# Get character n-grams (n=1-3) for a given tweet.\n",
    "    char_n_grams = getcharngrams(tweet)\n",
    "\n",
    "    stop_words = get_stop_words()\n",
    "# Replace emoticons, hashtags, mentions and URLs in a tweet.\n",
    "    processed_tokenized_tweet = processtweet(tokenized_tweet, stop_words)\n",
    "    processed_tweet = \" \".join(processed_tokenized_tweet[0:])\n",
    "\n",
    "# Get word n-grams (n=1-5) for the tweet.\n",
    "    word_n_grams = getwordngrams(processed_tokenized_tweet)\n",
    "\n",
    "    return emoticons, hashtags, mentions, urls, char_n_grams, word_n_grams\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

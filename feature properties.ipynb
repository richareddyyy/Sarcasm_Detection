{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from extract_tweets import get_id_truth_map, get_tweet_map, get_id_tokenised_tweet_map, get_token_lang_map, get_stop_words\n",
    "from preprocessing import gethashtags, getwordngrams, getcharngrams, processtweet\n",
    "\n",
    "char_n_grams_index = {}\n",
    "word_n_grams_index = {}\n",
    "\n",
    "def getallcharngrams(id_tweet_map):\n",
    "    char_n_grams = []\n",
    "    n_grams_count = {}\n",
    "    for key, tweet in id_tweet_map.iteritems():\n",
    "        char_i_grams = getcharngrams(tweet)\n",
    "\n",
    "        for i_gram in char_i_grams:\n",
    "            if i_gram in n_grams_count:\n",
    "                n_grams_count[i_gram] += 1\n",
    "            else:\n",
    "                n_grams_count[i_gram] = 1\n",
    "\n",
    "    for i_gram, count in n_grams_count.iteritems():\n",
    "        if count >= 8:\n",
    "            char_n_grams.append(i_gram)\n",
    "    return char_n_grams\n",
    "\n",
    "def getcharngramsindex(char_n_grams):\n",
    "    count = 0\n",
    "\n",
    "    for i_gram in char_n_grams:\n",
    "        char_n_grams_index[i_gram] = count\n",
    "        count += 1\n",
    "\n",
    "def getallwordngrams(id_tweet_map, id_tokenised_tweet_map):\n",
    "    word_n_grams = []\n",
    "    n_grams_count = {}\n",
    "    for key, tweet in id_tweet_map.iteritems():\n",
    "        tokenized_tweet = id_tokenised_tweet_map[key]\n",
    "        word_i_grams = getwordngrams(tokenized_tweet)\n",
    "\n",
    "    for i_gram in word_i_grams:\n",
    "            if i_gram in n_grams_count:\n",
    "                n_grams_count[i_gram] += 1\n",
    "            else:\n",
    "                n_grams_count[i_gram] = 1\n",
    "\n",
    "    for i_gram, count in n_grams_count.iteritems():\n",
    "        if count >= 10:\n",
    "            word_n_grams.append(i_gram)\n",
    "    return word_n_grams\n",
    "\n",
    "def getwordngramsindex(word_n_grams):\n",
    "    count = 0\n",
    "# print len(word_n_grams)\n",
    "    for i_gram in word_n_grams:\n",
    "        word_n_grams_index[i_gram] = count\n",
    "        count += 1\n",
    "\n",
    "def processtweetforwordngrams(id_tweet_map, id_tokenised_tweet_map):\n",
    "    processed_id_tweet_map = {}\n",
    "    processed_id_tokenised_tweet_map = {}\n",
    "    stop_words = get_stop_words()\n",
    "    for key, tweet in id_tweet_map.iteritems():\n",
    "        tokenized_tweet = id_tokenised_tweet_map[key]\n",
    "\n",
    "# Replace emoticons, hashtags, mentions and URLs in a tweet and remove punctuations.\n",
    "        processed_tokenized_tweet = processtweet(tokenized_tweet, stop_words)\n",
    "        processed_tweet = \" \".join(processed_tokenized_tweet[0:])\n",
    "\n",
    "        processed_id_tweet_map[key] = processed_tweet\n",
    "        processed_id_tokenised_tweet_map[key] = processed_tokenized_tweet\n",
    "\n",
    "    return processed_id_tweet_map, processed_id_tokenised_tweet_map\n",
    "\n",
    "def gettargethashtags(hashtag_count, hashtags, index_truth):\n",
    "    for hashtag in hashtags:\n",
    "        hashtag = hashtag.lower()\n",
    "        if hashtag in hashtag_count[index_truth]:\n",
    "            hashtag_count[index_truth][hashtag] += 1\n",
    "        else:\n",
    "            hashtag_count[index_truth][hashtag] = 1\n",
    "    return hashtag_count\n",
    "\n",
    "def gettargettokens(token_count, processed_tokenized_tweet, index_truth):\n",
    "    for token in processed_tokenized_tweet:\n",
    "        token = token.lower()\n",
    "        if token in ['hashtag', 'url', 'mention', 'emoticon']:\n",
    "            continue\n",
    "        if token in token_count[index_truth]:\n",
    "            token_count[index_truth][token] += 1\n",
    "        else:\n",
    "            token_count[index_truth][token] = 1\n",
    "    return token_count\n",
    "\n",
    "# Get count of all the hashtags and tokens for each of the class of gender and truth.\n",
    "def gettargetwords(id_tweet_map, processed_id_tweet_map, id_tokenised_tweet_map, processed_id_tokenised_tweet_map, id_truth_map):\n",
    "    hashtag_count = [{}, {}]\n",
    "    token_count = [{}, {}]\n",
    "\n",
    "    index = {}\n",
    "    index['YES'] = 0\n",
    "    index['NO'] = 1\n",
    "    for key, tweet in id_tweet_map.iteritems():\n",
    "        tokenized_tweet = id_tokenised_tweet_map[key]\n",
    "        hashtags = gethashtags(tokenized_tweet)\n",
    "        hashtag_count = gettargethashtags(hashtag_count, hashtags, index[id_truth_map[key]])\n",
    "\n",
    "        processed_tokenized_tweet = processed_id_tokenised_tweet_map[key]\n",
    "        token_count = gettargettokens(token_count, processed_tokenized_tweet, index[id_truth_map[key]])\n",
    "    \n",
    "    return token_count, hashtag_count\n",
    "\n",
    "# Get hashtags with highest count for each of the class of gender and truth.\n",
    "def gettophashtags(hashtag_count):\n",
    "    truth_top_hashtags = []\n",
    "\n",
    "    # Get hashtags for truth which have a score of >= 0.4.\n",
    "    for i in xrange(0,2):\n",
    "        for hashtag, value in hashtag_count[i].iteritems():\n",
    "            c1 = float(value)\n",
    "            total = 0.0\n",
    "            if hashtag in hashtag_count[0]:\n",
    "                total += float(hashtag_count[0][hashtag])\n",
    "            if hashtag in hashtag_count[1]:\n",
    "                total += float(hashtag_count[1][hashtag])\n",
    "\n",
    "            if float(c1/total) >= 0.6 and total >= 5:\n",
    "                truth_top_hashtags.append(hashtag)\n",
    "\n",
    "    truth_top_hashtags = set(truth_top_hashtags)\n",
    "    truth_top_hashtags = list(truth_top_hashtags)\n",
    "\n",
    "    return truth_top_hashtags\n",
    "\n",
    "\n",
    "def gettoptokens(token_count):\n",
    "    truth_top_tokens = []\n",
    "\n",
    "    # Get hashtags for gender which have a score of >= 0.4.\n",
    "    for i in xrange(0,2):\n",
    "        for token, value in token_count[i].iteritems():\n",
    "            c1 = float(value)\n",
    "            total = 0.0\n",
    "            if token in token_count[0]:\n",
    "                total += float(token_count[0][token])\n",
    "            if token in token_count[1]:\n",
    "                total += float(token_count[1][token])\n",
    "            if float(c1/total) >= 0.6 and total >= 5:\n",
    "                truth_top_tokens.append(token)\n",
    "\n",
    "    token_lang_map = get_token_lang_map()\n",
    "\n",
    "    truth_top_tokens = set(truth_top_tokens)\n",
    "    truth_top_tokens = list(truth_top_tokens)\n",
    "\n",
    "    truth_top_hi_tokens = []\n",
    "    truth_top_en_tokens = []\n",
    "    truth_top_rest_tokens = []\n",
    "\n",
    "    for token in truth_top_tokens:\n",
    "        if token.lower() not in token_lang_map:\n",
    "            truth_top_rest_tokens.append(token)\n",
    "        elif token_lang_map[token.lower()] == 'hi':\n",
    "            truth_top_hi_tokens.append(token)\n",
    "        elif token_lang_map[token.lower()] == 'en':\n",
    "            truth_top_en_tokens.append(token)\n",
    "        elif token_lang_map[token.lower()] == 'rest':\n",
    "            truth_top_rest_tokens.append(token)\n",
    "\n",
    "    return truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens\n",
    "\n",
    "\n",
    "def findfeatureproperties():\n",
    "    id_tweet_map, tweet_id_map = get_tweet_map()\n",
    "    id_tokenised_tweet_map = get_id_tokenised_tweet_map()\n",
    "    id_truth_map = get_id_truth_map()\n",
    "\n",
    "# Get all char n-grams (n=1-5) from training set and create an index for each of them.\n",
    "    char_n_grams = getallcharngrams(id_tweet_map)\n",
    "    getcharngramsindex(char_n_grams)\n",
    "\n",
    "    processed_id_tweet_map, processed_id_tokenised_tweet_map = processtweetforwordngrams(id_tweet_map, id_tokenised_tweet_map)\n",
    "\n",
    "# Get all word n-grams (n=1-3) from training set and create an index for each of them.\n",
    "    word_n_grams = getallwordngrams(processed_id_tweet_map, processed_id_tokenised_tweet_map)\n",
    "    getwordngramsindex(word_n_grams)\n",
    "\n",
    "    token_count, hashtag_count = gettargetwords(id_tweet_map, processed_id_tweet_map, id_tokenised_tweet_map, processed_id_tokenised_tweet_map, id_truth_map)\n",
    "\n",
    "    truth_top_hashtags = gettophashtags(hashtag_count)\n",
    "    truth_top_hi_tokens, truth_top_en_tokens, truth_top_rest_tokens = gettoptokens(token_count)\n",
    "\n",
    "    fp = open('data.txt', 'w')\n",
    "    pickle.dump(6, fp)\n",
    "    pickle.dump(char_n_grams_index, fp)\n",
    "    pickle.dump(word_n_grams_index, fp)\n",
    "    pickle.dump(truth_top_hashtags, fp)\n",
    "    pickle.dump(truth_top_hi_tokens, fp)\n",
    "    pickle.dump(truth_top_en_tokens, fp)\n",
    "    pickle.dump(truth_top_rest_tokens, fp)\n",
    "    fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
